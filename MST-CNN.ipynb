{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6c155a",
   "metadata": {},
   "source": [
    "# 本文提出的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a7da6",
   "metadata": {},
   "source": [
    "## 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e043d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#reload(sys)\n",
    "import imp\n",
    "imp.reload(sys)\n",
    "import importlib\n",
    "importlib.reload(sys)\n",
    "#import cPickle as pkl\n",
    "import _pickle as pkl\n",
    "import _pickle as cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "import time, datetime\n",
    "import re\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, concatenate, add\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate, Layer, Embedding, LSTM, Dense,Attention\n",
    "from keras import initializers\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import *\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "#from tsne import tsne\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import common_texts, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae9ed8",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5730144",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='embedconv training') #建立解析对象\n",
    "parser.add_argument('-batchsize', dest='batchsize', type=int, default=128, help='size of one batch')#超参数\n",
    "parser.add_argument('-init', dest='init', action='store_true', default=True, help='initialize vector')\n",
    "parser.add_argument('-noinit', dest='init', action='store_false', help='no initialize')\n",
    "parser.add_argument('-trainable', dest='trainable', action='store_true', default=True, help='embedding vectors trainable')\n",
    "parser.add_argument('-notrainable', dest='trainable', action='store_false', help='not trainable')\n",
    "parser.add_argument('-transform', dest='transform', action='store_true', default=True, help='transformation of the cost')\n",
    "parser.add_argument('-test', dest='test', action='store_true', default=False, help='only test step')\n",
    "parser.add_argument('-filter', dest='filter', action='store_true', default=True, help='filter rare codes')\n",
    "parser.add_argument('-isdays', dest='isdays', action='store_true', default=False, help='prediction of length of stay')\n",
    "parser.add_argument('-dropout', dest='dpt', type=float, default=0.01, help='drop out rate')#超参数\n",
    "parser.add_argument('-filtersize', dest='fz', type=int, default=3, help='filter region size')\n",
    "parser.add_argument('-filternumber', dest='fn', type=int, default=100, help='filter numbers')#\n",
    "parser.add_argument('-lr', dest='lr', type=float, default=0.001, help='learning rate' )#超参数\n",
    "parser.add_argument('-maxlen', dest='maxlen', type=int, default=17, help='max sequence length')\n",
    "parser.add_argument('-dim', dest='dim', type=int, default=600, help='embedding vector length')#\n",
    "parser.add_argument('-window', dest='window', type=int, default=10, help='word2vec window size')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "dropout = args.dpt\n",
    "filter_size = args.fz\n",
    "filter_number = args.fn\n",
    "lr = args.lr\n",
    "embedding_vector_length = args.dim\n",
    "print('embedding vector length %d'%(embedding_vector_length))\n",
    "\n",
    "if args.test:\n",
    "    MID = 29\n",
    "    SID = 48\n",
    "else:\n",
    "    i = datetime.datetime.now()\n",
    "    MID = i.minute\n",
    "    SID = i.second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa57a6",
   "metadata": {},
   "source": [
    "## 步骤1:数据的清洗和导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c158b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/content/gdrive/MyDrive/15684_new_2.csv'\n",
    "p = r'^[A-Z]'\n",
    "pattern = re.compile(p) #将一个正则表达式编译成 Pattern 对象，可以利用 pattern 的一系列方法对文本进行匹配查找\n",
    "\n",
    "def DataClean ():\n",
    "    print('开始步骤1:数据的清洗和导入')\n",
    "    data = pd.read_csv('/content/gdrive/MyDrive/15684_new_2.csv', encoding='utf-8')\n",
    "    data = data[['target','Hospital_days', #预测目标值\n",
    "                'Historical_days','Age','Hospital_times','Historical_times','time_interval','Date','Year','Month','Week','Historical_diagnoses',#患者特征\n",
    "                'Diagnostic_code6']] #疾病特征\n",
    "    data = data.dropna(subset=['Historical_days','Age','Hospital_times','Diagnostic_code6',]) #该函数主要用于滤除缺失数据\n",
    "    cPickle.dump(data, open('/content/gdrive/MyDrive/dataclean.df', 'wb')) ##把data写入到后面链接文件中\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c086a",
   "metadata": {},
   "source": [
    "## 步骤2:形成原始单词清单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648894a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToRawList(data):\n",
    "    print('开始步骤2:形成原始单词清单')\n",
    "    n_samples = len(data.index)\n",
    "    # demographics, P27=days\n",
    "    demographics = np.zeros((n_samples, 10))\n",
    "    demographics[:, 0:1] = data[['Historical_days']].values \n",
    "    demographics[:, 1:2] = data[['Age']].values \n",
    "    demographics[:, 2:3] = data[['Hospital_times']].values\n",
    "    demographics[:, 3:4] = data[['Historical_times']].values\n",
    "    demographics[:, 4:5] = data[['Date']].values\n",
    "    demographics[:, 5:6] = data[['Year']].values\n",
    "    demographics[:, 6:7] = data[['Month']].values\n",
    "    demographics[:, 7:8] = data[['Week']].values\n",
    "    demographics[:, 8:9] = data[['Historical_diagnoses']].values\n",
    "    #demographics[:, 9:10] = data[['time_interval']].values\n",
    "    #diseases codes\n",
    "    disease = data[['Diagnostic_code6']]\n",
    "    disease = disease.fillna('')\n",
    "    disease = disease.values\n",
    "    disease = [[str(code).strip() for code in item if code != ''] for item in disease]\n",
    "    #print(disease[:5])\n",
    "    main_dis = data[['Diagnostic_code6']]\n",
    "    main_dis = main_dis.fillna('')\n",
    "    main_dis = main_dis.values\n",
    "    l_disease = [len(item) for item in disease]\n",
    "    l_disease = np.array(l_disease)\n",
    "    C_disease = Counter(l_disease)\n",
    "    #print(\"l_disease的最大长度:{}, 最小长度:{}\".format(np.max(l_disease),np.min(l_disease))) \n",
    "    #print(\"l_disease的the 25% quarter:{}, the 75% quarter:{}, the mean:{}(+-){}\".format(np.percentile(l_disease,25),np.percentile(l_disease,75),np.mean(l_disease),np.std(l_disease))) \n",
    "    l_disease = np.array(l_disease)\n",
    "\n",
    "    seqs = data[['Historical_days','Age','Hospital_times','Historical_times','time_interval','Date','Year','Month','Week','Historical_diagnoses','Diagnostic_code6']]\n",
    "    #seqs = data[['Diagnostic_code6']]\n",
    "    seqs = seqs.fillna('')\n",
    "    seqs = seqs.values\n",
    "    seqs = [['#'.join(str(code).strip().split(' ')) for code in item if code != ''] for item in seqs]#replace the space with '#''\n",
    "    seqs = [[str('0'+code) if len(code.split('.')[0])==1 else code for code in item] for item in seqs] #replace '3.90034' with the '03.90034'\n",
    "    #print(seqs[:5])\n",
    "\n",
    "    cost = data[['target']].values\n",
    "    cost = np.asarray(cost, dtype=np.float32)\n",
    "    days = data[['Hospital_days']].values\n",
    "    days = np.asarray(days, dtype=np.float32)\n",
    "\n",
    "    n_samples = len(seqs) #返回疾病编码的个数，包括相同值，即12548/15684\n",
    "    #print(f,'#samples(seqs的长度):%d'%(n_samples)) # %d：以十进制形式输出带符号整数(正数不输出符号)\n",
    "\n",
    "    main_dis = [[str(code) for code in item if code != ''] for item in main_dis] #输出每个单词\n",
    "    C_maincodes = Counter([code for seq in main_dis for code in seq]) #计数，求每个单词出现的次数\n",
    "    main_code = C_maincodes.keys() #形成单词字典\n",
    "    n_dim = len(main_code) #共有242个单词\n",
    "    #print(\"len(main_code):{}\".format(n_dim))\n",
    "\n",
    "    code2id = dict(zip(main_code, range(n_dim))) #zip：打包为元组的列表，相对于编号\n",
    "    maincodemat = np.zeros((n_samples, n_dim), dtype=np.float32) #形成12548*242的0矩阵\n",
    "    for i in range(n_samples):\n",
    "        for code in main_dis[i]:\n",
    "            if code in code2id:\n",
    "                index = code2id[code]\n",
    "                maincodemat[i,index] += 1\n",
    "    #print(\"maincodemat.shape:{}\".format(maincodemat.shape)) #形成单词矩阵maincodemat，出现，则该行为1\n",
    "    Data = (seqs, cost, days, demographics,disease)\n",
    "    pkl.dump(Data, open('/content/gdrive/MyDrive/Data.df','wb'))\n",
    "    return seqs, cost, days, demographics, disease, main_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0d3f8",
   "metadata": {},
   "source": [
    "## 步骤3:给单词编号,形成单词-编号的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤3:给单词编号\n",
    "def token_to_index(seqs):\n",
    "    print('开始步骤3:给单词编号,形成单词-编号的字典')\n",
    "    C_codes = Counter([code for seq in seqs for code in seq])\n",
    "    code_index = {}\n",
    "    for idx, item in enumerate(C_codes.keys()): #enumerate：同时列出数据、数据下标\n",
    "        code_index[item] = idx + 1\n",
    "    #print(\"the unique codes: {}\".format(len(C_codes.keys())))\n",
    "    return code_index   #输入code_index，输出：{'E10.901': 1,...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d5116",
   "metadata": {},
   "source": [
    "## 步骤4:词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0131ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_embedding(code_index={}, level=0):\n",
    "    print(\"开始第4步\")\n",
    "    index_embedding = {}\n",
    "    cnt = 0\n",
    "    dim = 600\n",
    "    window = 10    #glove文件包含各种大小的文本编码向量:50维、100维、200维、300维\n",
    "    #model = KeyedVectors.load_word2vec_format(word_vector_path, binary=True)\n",
    "    model = KeyedVectors.load('/content/gdrive/MyDrive/newlevel%d_word2vec_dim%d_window%d.model' %(level, dim, window))\n",
    "   \n",
    "    for code, index in code_index.items():  # items(): 返回可遍历的(键, 值) 元组数组\n",
    "        #print(re.findall(pattern, code))\n",
    "        if len(re.findall(pattern, code))== 1:\n",
    "            if level == 0:\n",
    "                newcode = code\n",
    "            if level == 1:\n",
    "                newcode = code[0:3]\n",
    "            if level == 2:\n",
    "                newcode = code[0:5]\n",
    "            if level == 3:\n",
    "                newcode = code[0:6]\n",
    "        else:\n",
    "            if level == 0:\n",
    "                newcode = code\n",
    "            if level == 1:\n",
    "                newcode = code[0:3]\n",
    "            if level == 2:\n",
    "                newcode = code[0:4]\n",
    "            if level == 3:\n",
    "                newcode = code[0:5]\n",
    "\n",
    "        if newcode in model:\n",
    "            index_embedding[index] = model[newcode]\n",
    "        else:\n",
    "            cnt = cnt + 1\n",
    "            index_embedding[index] = np.random.uniform(-0.25,0.25,embedding_vector_length)\n",
    "    return index_embedding #即619个疾病代码，每个代码映射成了100维的向量\n",
    "\n",
    "def get_trained_embedding(index_embedding=None):\n",
    "    print(\"开始get_trained_embedding\")\n",
    "    index_sorted = sorted(index_embedding.items()) # sorted by index starting from 1 ;items(): 返回可遍历的(键, 值) 元组数组\n",
    "    trained_embedding = [t[1] for t in index_sorted] #len(trained_embedding):424 type:<class 'list'>\n",
    "    embedding_vector_length = args.dim\n",
    "    zeros = np.random.uniform(-0.25,0.25,embedding_vector_length) #一维的数组  zeros.shape:(424,)\n",
    "    trained_embedding = np.vstack((zeros, trained_embedding)) #它是垂直（按照行顺序）的把数组给堆叠起来。\n",
    "    trained_embedding = np.array(trained_embedding) #trained_embedding.shape:(101, 424);trained_embedding.维度:2\n",
    "    return trained_embedding\n",
    "\n",
    "#步骤5:单尺度编码 E10.901 C12.783\n",
    "def embedding_encoder(idseqs, index_embedding):\n",
    "    print('开始步骤51/3:单尺度编码')\n",
    "    n_samples = len(idseqs)\n",
    "    dim = args.dim\n",
    "    mat = np.zeros((n_samples, dim), np.float32)\n",
    "    for i in range(n_samples):\n",
    "        for codeid in idseqs[i]:\n",
    "            if codeid in index_embedding:\n",
    "                code_embedding = np.array(index_embedding[codeid])\n",
    "            else:\n",
    "                code_embedding = np.random.uniform(-0.25, 0.25, dim)\n",
    "            mat[i] += code_embedding\n",
    "    mat = np.array(mat)\n",
    "    return mat\n",
    "\n",
    "#多尺度编码 E10.901 C12.783     E10.90 C12.78  E10.9      6-5-4-3\n",
    "def multichannel_embedding_encoder(idseqs, index_embedding,index_embedding1,index_embedding2,index_embedding3):\n",
    "    print('开始步骤52/3:开始多尺度编码...')\n",
    "    n_samples = len(idseqs)\n",
    "    dim = args.dim\n",
    "    mat = np.zeros((n_samples, dim), np.float32)\n",
    "    for i in range(n_samples):\n",
    "        for codeid in idseqs[i]:\n",
    "            code_embedding = np.asarray([0]*dim, np.float32)\n",
    "            if codeid in index_embedding:\n",
    "                code_embedding += np.array(index_embedding[codeid])\n",
    "            if codeid in index_embedding1:\n",
    "                code_embedding += np.array(index_embedding1[codeid])\n",
    "            if codeid in index_embedding2:\n",
    "                code_embedding += np.array(index_embedding2[codeid])\n",
    "            if codeid in index_embedding3:\n",
    "                code_embedding += np.array(index_embedding3[codeid])\n",
    "            mat[i] += code_embedding\n",
    "    mat = np.array(mat) \n",
    "    return mat\n",
    "\n",
    "#独热码\n",
    "def one_hot_encoder(seqsind, nb_words):\n",
    "    print('开始步骤53/3:开始独热编码one_hot encoding...')\n",
    "    n_samples = len(seqsind)\n",
    "    n_dim = nb_words\n",
    "    mat = np.zeros((n_samples, n_dim), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        for codeid in seqsind[i]:\n",
    "            if codeid !=0:\n",
    "                index = codeid - 1 # for the seqid starts from 1\n",
    "                mat[i,index] = mat[i,index] + 1\n",
    "    return mat\n",
    "\n",
    "#步骤6:\n",
    "def svd(seqsind, dim=args.dim): #600 奇异值分解(SVD)\n",
    "    print(\"开始61/3:开始svd\")\n",
    "    mat = one_hot_encoder(seqsind, nb_words)\n",
    "    #print(\"mat.shape:{}\".format(mat.shape))\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    print('SVD......') \n",
    "    svd.fit(mat)\n",
    "    res = svd.transform(mat)\n",
    "    print('SVD done!') \n",
    "    return res\n",
    "\n",
    "def glove(level, dim=args.dim, window=10): \n",
    "    print(\"开始62/3:开始glove\")\n",
    "    vectors = '/content/gdrive/MyDrive/weighted1_vectors%d'%(level)\n",
    "    model = KeyedVectors.load_word2vec_format(vectors, binary=False)\n",
    "    name = 'newlevel%d_weighted1_glove_dim%d_window%d.model'%(level, dim, window)\n",
    "    model.save('/content/gdrive/MyDrive/cnn_model/'+name)\n",
    "    return model\n",
    "\n",
    "def plot_embedding(data, label, title):\n",
    "    x_min, x_max = np.min(data, 0), np.max(data, 0)\n",
    "    data = (data - x_min) / (x_max - x_min)\n",
    " \n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.text(data[i, 0], data[i, 1], str(label[i]),\n",
    "                 color=plt.cm.Set1(label[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded892a4",
   "metadata": {},
   "source": [
    "## 步骤5:加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2821c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤7:加载数据\n",
    "def load_data(X,demographics, y, onehot_mat):\n",
    "    print('开始步骤7:划分训练集、验证集、测试集...')\n",
    "    print(\"y:\",y)\n",
    "    #y=np.log(y)\n",
    "    indices = np.arange(n_seqs)\n",
    "    np.random.shuffle(indices) #打乱\n",
    "    X = X[indices]\n",
    "    demographics = demographics[indices]\n",
    "    y = y[indices]\n",
    "    #onehot_mat = onehot_mat[indices]\n",
    "\n",
    "    n_tr = int(n_seqs * 0.85)\n",
    "    n_va = int(n_seqs * 0.05)\n",
    "    n_te = n_seqs - n_tr - n_va\n",
    "    X_train = X[:n_tr]\n",
    "    demographics_train = demographics[:n_tr]\n",
    "    y_train = y[:n_tr]\n",
    "    #onehot_mat_train = onehot_mat[:n_tr]\n",
    "\n",
    "    X_valid = X[n_tr:n_tr+n_va]\n",
    "    demographics_valid = demographics[n_tr:n_tr+n_va]\n",
    "    y_valid = y[n_tr:n_tr+n_va]\n",
    "    #onehot_mat_valid = onehot_mat[n_tr:n_tr+n_va]\n",
    "\n",
    "    X_test = X[-n_te:]\n",
    "    demographics_test = demographics[-n_te:]\n",
    "    y_test = y[-n_te:]\n",
    "    #onehot_mat_test = onehot_mat[-n_te:]\n",
    "\n",
    "    #print np.max(y_test),np.min(y_test)\n",
    "    return X_train, X_test, X_valid, y_train, y_test, y_valid, demographics_train, demographics_test, demographics_valid#, onehot_mat_train,onehot_mat_test,onehot_mat_valid\n",
    "\n",
    "\n",
    "def filter_test(X_test, index_code, threshold=2):\n",
    "    print('开始步骤8:选择罕见集样本用于测试集...，这一步其实是将所有数据15684都用于了测试集')\n",
    "    comm_inds = []\n",
    "    for ind, seq in enumerate(X_test):\n",
    "        cnt = 0\n",
    "        for index in seq:\n",
    "            if index in index_code:\n",
    "                code = index_code[index]\n",
    "            else:\n",
    "                code = 'none'\n",
    "        if cnt < threshold:\n",
    "            comm_inds.append(int(ind))\n",
    "    print('comm_inds:%d'%(len(comm_inds)))\n",
    "    comm_inds = np.asarray(comm_inds)\n",
    "    return  comm_inds\n",
    "\n",
    "def evaluation(y_test, y_pred):\n",
    "    print('y_test:{}'.format(y_test.shape))\n",
    "    print('y_pred:{}'.format(y_pred.shape))\n",
    "    print(y_pred.dtype) #float32\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print('MSE:{}, RMSE:{}, MAE:{}, R2:{}'.format(mse, rmse, mae,r2)) \n",
    "    return r2, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c36f7",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤8:预测、可视化\n",
    "def cross_validation():\n",
    "    print(\"开始5折交叉验证\")\n",
    "    estimator = KerasRegressor(build_fn=cnn_model, epochs=100, batch_size=args.batchsize, verbose=0)\n",
    "    kfold = KFold(n_splits=5, random_state=seed)\n",
    "    res = cross_val_score(estimator, X, y, cv=kfold)\n",
    "    print( 'cross validation for cnn model ......')\n",
    "    print('cnn_model: MSE %.2f (+-)%.2f'%(res.mean(), res.std())) \n",
    "#cross_validation()\n",
    "def daysplots(y_test, y_pred, r2, rmse, name):\n",
    "    print(\"开始步骤91/2:预测可视化-住院天数\")\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.set_xlim(0, 35)\n",
    "    #ax.set_ylim(0, 35)\n",
    "    #ax.plot(y_test.squeeze(), y_test.squeeze(), s=10, marker='.',c='r')\n",
    "    ax.scatter(y_test.squeeze(), y_pred, s=10,marker='.', c='b')\n",
    "    ax.text(28,30,'R2:{:.4f}'.format(r2))\n",
    "    ax.text(28,32,'RMSE:{:.2f}'.format(rmse))\n",
    "    ax.set_xlabel('True length of stay (days)')\n",
    "    ax.set_ylabel('predicted length of stay (days)')\n",
    "    fig.savefig('/content/gdrive/MyDrive/%s_days.pdf'%(name))\n",
    "\n",
    "def costplots(y_test, y_pred, r2, rmse, name):\n",
    "    print(\"开始步骤92/2:预测可视化-医疗费用\")\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.set_xlim(0, 12000)\n",
    "    #ax.set_ylim(0, 12000)\n",
    "    ax.scatter(y_test.squeeze(), y_pred, s=10,marker='o',c='b') #c='b'\n",
    "   # ax.text('R2:{:.4f}'.format(r2)) #8000,10000,\n",
    "    #ax.text('RMSE:{:.2f}'.format(rmse)) #8000,11000,\n",
    "    ax.set_xlabel('Series')\n",
    "    ax.set_ylabel('Medical cost')\n",
    "    fig.savefig('/content/gdrive/MyDrive/%s_cost.png'%(name),dpi=500,bbox_inches = 'tight')\n",
    "\n",
    "    #画散点图\n",
    "    plt.figure()#figsize=(15,5)\n",
    "    plt.plot(y_test,'rs',label='True value')\n",
    "    plt.plot(y_pred,'go',label='Predict value')\n",
    "    #plt.title('Predicted and true values of medical costs',fontsize=13)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.xlabel('Series',fontsize=13)\n",
    "    plt.ylabel('Medical Cost',fontsize=13)\n",
    "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
    "    plt.savefig('/content/gdrive/MyDrive/%s_散点图.png'%(name),dpi=500,bbox_inches = 'tight')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(y_test, linewidth = 1,linestyle='-',label='True Value') #marker='*'\n",
    "    plt.plot(y_pred, linewidth = 1,linestyle='-',label='Predicted Value') #marker='^',\n",
    "    #ax.text('R2:{:.4f}'.format(r2)) #8000,10000,\n",
    "    #ax.text('RMSE:{:.2f}'.format(rmse)) #8000,11000,\n",
    "    plt.xlabel('Series',fontsize=13)\n",
    "    plt.ylabel('Medical Cost',fontsize=13)\n",
    "    plt.xlabel('Series',fontsize=13)\n",
    "    plt.ylabel('Medical Cost',fontsize=13)\n",
    "    fig.savefig('/content/gdrive/MyDrive/%s_曲线图.pdf'%(name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patientvec(model,modelpath, disease,  demographics):\n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),by_name=False)\n",
    "    sub_model1 = Model(inputs=model.inputs, outputs=model.get_layer('F1').output)#错误代码修改\n",
    "    sub_model2 = Model(inputs=model.inputs, outputs=model.get_layer('F2').output)#错误代码修改\n",
    "    sub_model3 = Model(inputs=model.inputs, outputs=model.get_layer('F3').output)#错误代码修改\n",
    "    sub_model4 = Model(inputs=model.inputs, outputs=model.get_layer('F4').output)#错误代码修改\n",
    "    sub_model5 = Model(inputs=model.inputs, outputs=model.get_layer('F5').output)#错误代码修改\n",
    "\n",
    "    patientvecs3 = sub_model3.predict([disease,  demographics], verbose=1)\n",
    "    modelname='sub_model3'\n",
    "    print(patientvecs3.shape)\n",
    "    pkl.dump(patientvecs3, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))\n",
    "\n",
    "    patientvecs4 = sub_model4.predict([disease, demographics], verbose=1)\n",
    "    modelname='sub_model4'\n",
    "    print(patientvecs4.shape)\n",
    "    pkl.dump(patientvecs4, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))\n",
    "\n",
    "    patientvecs5 = sub_model5.predict([disease,  demographics], verbose=1)\n",
    "    modelname='sub_model5'\n",
    "    print(patientvecs5.shape)\n",
    "    pkl.dump(patientvecs5, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e54af",
   "metadata": {},
   "source": [
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBAM\n",
    "#导入注意力机制所需要的库\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, Reshape, Dense, multiply, Permute, Concatenate, Conv1D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "#1、通道注意力机制  具体来说他就是是对应一个全局平均池化的操作。将一个c通道，hxw的特征图，压成c通道1x1 可以看作一个C维向量\n",
    "#SENet采用一个小型的子网络，获得一组权重，进而将这组权重与各个通道的特征分别相乘，以调整各个通道特征的大小。\n",
    "#这个过程，就可以认为是在施加不同大小的注意力在各个特征通道上。\n",
    "#在SENet中，获得权重的具体路径是，“全局池化→全连接层→ReLU函数→全连接层→Sigmoid函数”。\n",
    "def channel_attention(input_feature, ratio=2):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1 #mage_data_format返回图像的维度顺序\n",
    "    print(\"input_feature.shape:{}\".format(input_feature.shape))\n",
    "    print(\"channel_axis:{}\".format(channel_axis)) #channel_axis:-1\n",
    "    channel = input_feature.shape[channel_axis] #这个作用就是找到通道所在的位置 shape[0]读取矩阵第一维度的长度 行数 ，shape[1]:列数，每行有几个元素\n",
    "    print(\"channel:{}\".format(channel)) #channel:1300\n",
    "\n",
    "    shared_layer_one = Dense(channel//ratio, #进行全连接，得到C/r维的向量\n",
    "                             kernel_initializer='he_normal', #He正态分布初始化方法 \n",
    "                             activation = 'relu', #prelu带参数的ReLU\n",
    "                             use_bias=True, #在进行Relu激活，再对进行一次全连接，将C/r维的向量变回C维向量，再进行sigmoid激活（使得数值位于0-1之间），这便是得到了权重矩阵。\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(input_feature) #输入（samples,steps,features）的3D张量，则输出形如（samples,features）的2D张量\n",
    "    avg_pool = Reshape((1,channel))(avg_pool) #如果是AveragePooling1D,则输入（samples,steps,features）的3D张量，则输出形如（samples,downsampled_steps,features）的3D张量\n",
    "    assert avg_pool.shape[1:] == (1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,channel//ratio) #C/r（r为减少率）\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,channel) #第二层神经元个数为 C，这个两层的神经网络是共享的。\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(input_feature)\n",
    "    max_pool = Reshape((1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool]) # 将MLP输出的特征进行基于element-wise的加和操作，再经过sigmoid激活操作，\n",
    "    cbam_feature = Activation('hard_sigmoid')(cbam_feature)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature) #Permute：可以同时多次交换tensor的维度\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature]) #最后，将M_c和输入特征图F做element-wise乘法操作，生成Spatial attention模块需要的输入特征\n",
    "\n",
    "#2、空间注意力机制\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 3 # #卷积核的尺寸 #实际上卷积大小为kernel_size* in_channels输入信号的通道,在文本分类中，即为词向量的维度\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature) #将tensor的维度换位。\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=2, keepdims=True))(cbam_feature) # 首先做一个基于channel的global max pooling \n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=2, keepdims=True))(cbam_feature) # 和global average pooling，得到两个H×W×1 的特征图\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=2)([avg_pool, max_pool]) #然后将这2个特征图基于channel 做concat操作（通道拼接）\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv1D(filters = 1, #filters 即为输出的维度       #然后经过一个7×7卷积，降维为1个channel\n",
    "                    kernel_size=kernel_size, #卷积核的尺寸\n",
    "                    activation = 'hard_sigmoid',  #再经过sigmoid生成spatial attention feature\n",
    "                    strides=1, #卷积步长\n",
    "                    padding='same', #输入的每一条边补充0的层数\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat) #如果bias=True，添加偏置\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3,1,2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature]) #最后将该feature和该模块的输入feature做乘法，得到最终生成的特征。 \n",
    "\n",
    "#3、构建CBAM\n",
    "def cbam_block(cbam_feature,ratio=2):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature, )\n",
    "    return cbam_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe283fc",
   "metadata": {},
   "source": [
    "## 多尺度划分——注意力模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ca2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四个模型：多尺度划分——注意力模型\n",
    "# se注意力机制 现有注意力模块的另一个重要影响因素：权值生成方法。现有注意力往往采用额外的子网络生成注意力权值，比如SE的GAP+FC+ReLU+FC+Sigmoid。\n",
    "def se_block(inputs, ratio=2):  # ratio代表第一个全连接层下降通道数的系数\n",
    "    in_channel = inputs.shape[-1]# 获取输入特征图的通道数\n",
    "    print('通道数:{}'.format(in_channel))\n",
    "    x = layers.GlobalAveragePooling1D()(inputs) # 全局平均池化[h,w,c]==>[None,c] #压缩：得到当前Feature Map的全局压缩特征量\n",
    "    x = layers.Reshape(target_shape=(1,1,in_channel))(x) # [None,c]==>[1,1,c]\n",
    "    x = layers.Dense(in_channel//ratio)(x)  # 全连接下降通道数 # [1,1,c]==>[1,1,c/4] #激发：通过两层全连接的bottleneck结构得到Feature Map中每个通道的权值\n",
    "    x = tf.nn.relu(x) # relu激活\n",
    "    x = layers.Dense(in_channel)(x)  # 全连接上升通道数 # [1,1,c/4]==>[1,1,c]\n",
    "    x = tf.nn.sigmoid(x) # sigmoid激活，权重归一化\n",
    "    print(\"sigmoid激活，权重归一化:\",x)\n",
    "    outputs = layers.multiply([inputs, x])  # 归一化权重和原输入特征图逐通道相乘 # [h,w,c]*[1,1,c]==>[h,w,c] #激发：并将加权后的Feature Map作为下一层网络的输入。\n",
    "    return outputs  \n",
    "#eca注意力机制：相比于se模块，实现了适当的跨通道交互而不是像全连接层一样全通道交互。（用一维卷积替换了全连接层）\n",
    "#通过执行卷积核大小为k的一维卷积来生成通道权重，其中k通过通道维度C的映射自适应地确定。\n",
    "import math\n",
    "def eca_block(inputs, b=1, gama=2):\n",
    "    in_channel = inputs.shape[-1] # 输入特征图的通道数\n",
    "    print('通道数:{}'.format(in_channel))\n",
    "    kernel_size = int(abs((math.log(in_channel, 2) + b) / gama)) # 根据公式计算自适应卷积核大小\n",
    "    if kernel_size % 2:              # 如果卷积核大小是偶数，就使用它\n",
    "        kernel_size = kernel_size\n",
    "    else:                            # 如果卷积核大小是奇数就变成偶数\n",
    "        kernel_size = kernel_size + 1\n",
    "    x = layers.GlobalAveragePooling1D()(inputs) # [h,w,c]==>[None,c] 全局平均池化\n",
    "    x = layers.Reshape(target_shape=(in_channel, 1))(x) # [None,c]==>[c,1]\n",
    "    x = layers.Conv1D(filters=1, kernel_size=kernel_size, padding='same', use_bias=False)(x) # [c,1]==>[c,1]\n",
    "    x = tf.nn.sigmoid(x) # sigmoid激活\n",
    "    x = layers.Reshape((1,1,in_channel))(x) # [c,1]==>[1,1,c]\n",
    "    outputs = layers.multiply([inputs, x]) # 结果和输入相乘\n",
    "    return outputs\n",
    "##\n",
    "def prelu(_x): #name=None\n",
    "    \"\"\"parametric ReLU activation\"\"\"\n",
    "    print(\"_x:\",_x.get_shape(),\"_x.get_shape()[-1]:\",_x.get_shape()[-1])\n",
    "    _alpha = tf.compat.v1.get_variable(\"prelu\", #name +\n",
    "                             shape=_x.get_shape()[-1], #get_shape():得到张量（数组）的维度\n",
    "                             dtype=_x.dtype,\n",
    "                             initializer=tf.constant_initializer(0.001))\n",
    "    pos = tf.nn.relu(_x)\n",
    "    neg = _alpha * (_x - tf.abs(_x)) * 0.5\n",
    "\n",
    "    return pos + neg\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def ATTENTION_multi_channel_split_model(demgras_dim):\n",
    "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
    "    dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix0],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding1level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix1],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding2level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix2],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding3level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix3],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    #bn = BatchNormalization() #BN层就是为了让让每一层的值在一个有效范围内传递下去。1、加快网络的训练和收敛的速度 2、控制梯度爆炸防止梯度消失 3、防止过拟合\n",
    "    #fusion_vector = concatenate([dis_embedding0level, dis_embedding1level, dis_embedding2level, dis_embedding3level])\n",
    "    conv_result = []\n",
    "    for i in range(3):\n",
    "        channel_result = []\n",
    "        #lstm = LSTM(units=1000, recurrent_activation='leaky_relu', dropout=0.1) \n",
    "        #lstm_out = lstm(fusion_vector)\n",
    "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
    "        conv0 = conv_layer(dis_embedding0level)\n",
    "        conv1 = conv_layer(dis_embedding1level)\n",
    "        conv2 = conv_layer(dis_embedding2level)\n",
    "        conv3 = conv_layer(dis_embedding3level)\n",
    "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
    "        pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "        pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "        pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "        channel_result.append(pooling0)\n",
    "        channel_result.append(pooling1)\n",
    "        channel_result.append(pooling2)\n",
    "        channel_result.append(pooling3)\n",
    "        allchannel = add(channel_result)\n",
    "        conv_result.append(allchannel)\n",
    "        #bn = BatchNormalization()\n",
    "        #conv_result.append(lstm_out)\n",
    "\n",
    "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
    "    print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:11\n",
    "    print(\"demgras_in.shape:{}\".format(demgras_in.shape))#(None, 11) sigmoid\n",
    "    dense_demgras = Dense(1000, activation='sigmoid')(demgras_in) #它将任意的值转换到 [0,1] 之间 BN层是将数据转换为均值为0，方差为1的正态分布\n",
    "    conv_result.append(dense_demgras)#append the demographics information\n",
    "    merge_out = concatenate(conv_result) #numpy.concatenate函数 主要作用:沿现有的某个轴对一系列数组进行拼接。\n",
    "    \n",
    "    merge_out = tf.expand_dims(merge_out, axis=0)\n",
    "    \n",
    "    x = se_block(merge_out)  # 接收SE返回值 #可行 但是平均误差突然间猛增 动荡 这个网络不错诶 虽然前期增加，但是后期整体趋势在下降，虽然偶尔动荡\n",
    "    x=tf.squeeze(x,[0, 1])\n",
    "\n",
    "    #x = eca_block(merge_out)  # 接收ECA输出结果 这个网络类似于上面这个网络 等会可以试试 和SENet相比大大减少了参数量，参数量等于一维卷积的kernel_size的大小\n",
    "    #x=tf.squeeze(x,[0, 1])\n",
    "    \n",
    "    #cbam = cbam_block(merge_out) #尝试把注意力机制放在这\n",
    "    #print(\"cbam:{}\".format(cbam.shape))\n",
    "    #x=tf.squeeze(cbam,[0])\n",
    "    #print(x.shape)\n",
    "    \n",
    "    dense_out = Dense(1000, activation='relu', name='F1')(merge_out)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(500, activation='relu', name='F2')(dpt) \n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(100, activation='relu', name='F3')(dpt) #RReLU中的aji是一个在一个给定的范围内随机抽取的值，这个值在测试环节就会固定下来。\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(50, activation='relu', name='F4')(dpt) #Leaky ReLU中的ai是固定的\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    #activation_function = keras.layers.advanced_activations.PReLU(init='zero', weights=None) \n",
    "    dense_out = Dense(2, activation='leaky_relu', name='F5')(dpt) #leaky_relu elu PReLU #PReLU中的ai是根据数据变化的#\n",
    "    #dense_out = prelu(dense_out)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    mode_out = Dense(1)(dpt)\n",
    "    model = Model([dis_in,demgras_in], mode_out) \n",
    "    #rmsprop = optimizers.rmsprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 均方根传播(RMSProp) ;根据最近的权重梯度的平均值(例如变化的速度)来调整;这意味着该算法在线上和非平稳问题上表现良好(如:噪声)。\n",
    "    Adam=tf.keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "    #rmsprop tf.keras.losses.Huber()\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae']) #tf.keras.losses.Huber()\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_ATTENTION_multi_channel_split_model(model, modelpath, X1_train,  demographics_train, y_train):\n",
    "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=8, verbose=1)#patience：能够容忍多少个epoch内都没有improvement。verbose：日志显示 verbose = 1 为输出进度条记录\n",
    "\n",
    "    print('Training the ATTENTION_merge model....') \n",
    "    history =model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[checkpointer,earlystopper], # 尝试关闭早停\n",
    "              verbose=1)\n",
    "    print(\"4、开始画图：损失\")\n",
    "    from matplotlib import rcParams\n",
    "    font2 = {'family': 'Times New Roman',\n",
    "             'weight': 'normal',\n",
    "               'size': 15,}\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss',fontsize=15)\n",
    "    plt.xlabel('Epoch',font2,verticalalignment='top') # fontsize=14, \n",
    "    plt.ylabel('Loss',font2, horizontalalignment='center') #fontsize=14, \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
    "    plt.savefig('/content/gdrive/MyDrive/LOSS.png',dpi=500,bbox_inches = 'tight')\n",
    " \n",
    "def test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    print(\"y_pred:{}\".format(y_pred.shape))\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    print(\"r2:\",r2,\"rmse:\",rmse)\n",
    "    #画图1\n",
    "    name = 'ATTENTION_MG_split%d'%(index)\n",
    "    costplots(y_test, y_pred, r2, rmse, name)\n",
    "    #画图2\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "    #画图3\n",
    "    print(\"!!开始画图：预测值与真实值\")\n",
    "    print(\"y_test:\",y_test.shape)\n",
    "    print(\"y_pred:\",y_pred.shape)\n",
    "    print(\"y_pred.ndim:\",y_pred.ndim)\n",
    "    print(\"y_test.ndim:\",y_test.ndim)\n",
    "    from matplotlib import rcParams\n",
    "    font2 = {'family': 'Times New Roman',\n",
    "             'weight': 'normal',\n",
    "               'size': 15,}\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(1569)),y_test,label='True Value')\n",
    "    plt.plot(list(range(1569)),y_pred,label='Predicted Value')\n",
    "    b=np.squeeze(y_test)\n",
    "    c=np.squeeze(y_pred)\n",
    "    plt.fill_between(list(range(1569)),b,c,color='g',alpha=.25)\n",
    "    plt.title('Predicted and true values of medical costs',fontsize=13)\n",
    "    plt.xlabel('Series',fontsize=13, verticalalignment='top') #\n",
    "    plt.ylabel('Medical cost',fontsize=13,horizontalalignment='center') #\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
    "    plt.savefig('/content/gdrive/MyDrive/Attention_曲线图.png',dpi=500,bbox_inches = 'tight')\n",
    "    #画散点图\n",
    "    plt.figure()#figsize=(15,5)\n",
    "    plt.plot(y_test,'rs',label='True value')\n",
    "    plt.plot(y_pred,'go',label='Predict value')\n",
    "    plt.title('Predicted and true values of medical costs',fontsize=13)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.xlabel('Series',fontsize=13,)\n",
    "    plt.ylabel('Medical Cost',fontsize=13)\n",
    "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
    "    plt.savefig('/content/gdrive/MyDrive/Attention_散点图.png',dpi=500,bbox_inches = 'tight')\n",
    "    #画：测试集的预测损失\n",
    "    plt.figure()\n",
    "    a=y_test-y_pred\n",
    "    plt.plot(a, label='Loss')\n",
    "    plt.title('Predicted loss of the proposed model',fontsize=13)\n",
    "    plt.xlabel('Series',fontsize=10, verticalalignment='top') #\n",
    "    plt.ylabel('Loss value',fontsize=10,horizontalalignment='center') #\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.savefig('/content/gdrive/MyDrive/Predicted_LOSS.png',dpi=500,bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b74ee",
   "metadata": {},
   "source": [
    "# 开始预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a8840",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'weighted_%s:%s_%sdays_%sinit_%strainable_%fdpt_%flr_%dfz_%dfn_%dmaxlen_%ddim_%sfilter_%stransform %sbatch_%swindow'%(MID, SID, args.isdays, args.init, args.trainable, args.dpt, args.lr, args.fz, args.fn, args.maxlen, args.dim, args.filter, args.transform,args.batchsize,args.window)\n",
    "\n",
    "if os.path.isfile('/content/gdrive/MyDrive/dataclean.df'):\n",
    "    data = pkl.load(open('/content/gdrive/MyDrive/dataclean.df','rb'))\n",
    "else:\n",
    "    data = DataClean()\n",
    "rawseqs, cost, days, demographics, disease, main_dis = ToRawList(data) #rawseqs就是seqs\n",
    "#print(\"rawseqs:\",rawseqs)\n",
    "\n",
    "seqs1levels = []\n",
    "seqs2levels = []\n",
    "seqs3levels = []\n",
    "for seqs in rawseqs:\n",
    "    levels1 = []\n",
    "    levels2 = []\n",
    "    levels3 = []\n",
    "    for code in seqs:\n",
    "        levels1.append(code[0:3])\n",
    "        if len(re.findall(pattern, code))== 1:\n",
    "            levels2.append(code[0:5])\n",
    "            levels3.append(code[0:6])\n",
    "        else:\n",
    "            levels2.append(code[0:4])\n",
    "            levels3.append(code[0:5])\n",
    "    seqs1levels.append(levels1)\n",
    "    seqs2levels.append(levels2)\n",
    "    seqs3levels.append(levels3)\n",
    "\n",
    "print('开始将疾病诊断代码转换为索引')\n",
    "code_index = token_to_index(rawseqs)# transform code into index 将代码转换为索引\n",
    "index_code = dict([(kv[1], kv[0]) for kv in code_index.items()])# validate the code_index 验证代码_索引\n",
    "#print [[index_code[index] for index in item ] for item in seqs[0:2]]\n",
    "\n",
    "idseqs = [[code_index[code] for code in item] for item in rawseqs]\n",
    "iddis = [[code_index[code] for code in item] for item in disease]\n",
    "idmain_dis = [[code_index[code] for code in item] for item in main_dis]\n",
    "#print(main_dis)\n",
    "#\"\"\"\n",
    "print('开始形成词嵌入向量')\n",
    "print(disease)\n",
    "print(disease[1:]) #list 15684\n",
    "word2vec_model(disease,0)\n",
    "index_embedding = get_index_embedding(code_index, 0)\n",
    "embedding_matrix0 = get_trained_embedding(index_embedding)\n",
    "word2vec_model(disease, 1)\n",
    "index_embedding1 = get_index_embedding(code_index, 1)\n",
    "embedding_matrix1 = get_trained_embedding(index_embedding1)\n",
    "word2vec_model(disease, 2)\n",
    "index_embedding2 = get_index_embedding(code_index, 2)\n",
    "embedding_matrix2 = get_trained_embedding(index_embedding2)\n",
    "word2vec_model(disease, 3)\n",
    "index_embedding3 = get_index_embedding(code_index, 3)\n",
    "embedding_matrix3 = get_trained_embedding(index_embedding3)\n",
    "\n",
    "main_code = []\n",
    "for item in idmain_dis:\n",
    "    for ind in item:\n",
    "        main_code.append(embedding_matrix0[1][ind])\n",
    "\n",
    "maincodemat = np.array(main_code)\n",
    "\n",
    "nb_words = len(index_code) # code_index starting from 1\n",
    "max_features = nb_words + 1 \n",
    "n_seqs = len(idseqs)\n",
    "\n",
    "MAX_LEN = args.maxlen      # the max len is 21 #这里没有治疗编码，所以最大长度为以记录为单位\n",
    "seqs = pad_sequences(idseqs, maxlen=MAX_LEN)\n",
    "DIS_MAX_LEN = 17  #其实这里需要改成以记录为单位才对 DIS_MAX_LEN = 11 SUR_MAX_LEN = 10 maxlen:21\n",
    "disease = pad_sequences(iddis, maxlen=DIS_MAX_LEN)\n",
    "\n",
    "\n",
    "X = np.array(seqs)\n",
    "X1 = np.array(disease)\n",
    "print(\"X(seqs):\",X.shape,\"X1(disease):\",X1.shape)\n",
    "print(\"seqs:\",seqs.shape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if args.test:\n",
    "    modelpath = 'MG_merge' + path\n",
    "    demgras_dim = 3\n",
    "    model= multi_channel_split_model(demgras_dim)\n",
    "    extract_patientvec(model, modelpath, disease, surgery, demographics)\n",
    "\"\"\"\n",
    "#demographics = np.hstack((demographics, maincodemat))\n",
    "if args.isdays:\n",
    "    y = np.asarray(days, dtype='float32')\n",
    "else:\n",
    "    y = np.asarray(cost,dtype='float32')\n",
    "\n",
    "if args.transform:\n",
    "    print(\"np.log\")\n",
    "    y = np.log(y)\n",
    "print(X, demographics)\n",
    "\n",
    "comm_inds= filter_test(X, index_code, 4)\n",
    "comm_X = X[comm_inds]\n",
    "comm_X1 = X1[comm_inds]\n",
    "comm_demographics = demographics[comm_inds]\n",
    "comm_y = y[comm_inds]\n",
    "print(\"comm_X:\",comm_X,\"comm_X1:\",comm_X1,\"comm_y:\",comm_y)\n",
    "\n",
    "print('Spliting train, test parts...')\n",
    "X_train, X_test, X1_train, X1_test, y_train, y_test, demographics_train, demographics_test = train_test_split(comm_X, comm_X1,comm_y, comm_demographics, test_size=0.1, random_state=90)\n",
    "print('X_train:{}'.format(X_train.shape))\n",
    "print('X_test:{}'.format(X_test.shape))\n",
    "print('X1_train:{}'.format(X1_train.shape))\n",
    "print('X1_test:{}'.format(X1_test.shape))\n",
    "print('y_train:{}'.format(y_train.shape))\n",
    "print('y_test:{}'.format(y_test.shape))\n",
    "print('demographics_train:{}'.format(demographics_train.shape))\n",
    "print('demographics_test:{}'.format(demographics_test.shape))\n",
    "\n",
    "#y_train=np.log(y_train)\n",
    "#y_test=np.log(y_test)\n",
    "print(\"y_test:\",y_test)\n",
    "  \n",
    "demgras_dim = demographics.shape[1] #图像的水平尺寸 \n",
    "print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cb9dd",
   "metadata": {},
   "source": [
    "## 开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================== 第四部分 开始预测 =====================================================\n",
    "r2_CNN_val=[]\n",
    "r2_CNN_test=[]\n",
    "for i in range(1):\n",
    "    print('第%s次训练'%i)\n",
    "  \n",
    "    modelpath = '9月1日(上午)_'+'第%s次训练_'%i+'MG_merge' + path # %i\n",
    "    #定义模型\n",
    "    model= ATTENTION_multi_channel_split_model(demgras_dim) \n",
    "    #划分数据\n",
    "    X1_train_1,X1_val_1, demographics_train_1,demographics_val_1,y_train_1,y_val_1=train_test_split( X1_train, demographics_train, y_train, test_size=0.1, random_state=90) \n",
    "    if not args.test:\n",
    "        train_ATTENTION_multi_channel_split_model(model, modelpath, X1_train_1, demographics_train_1, y_train_1)\n",
    "    extract_patientvec(model, modelpath, disease, demographics)\n",
    "    #拟合、预测\n",
    "    y_CNN_hat = model.predict([X1_val_1,demographics_val_1])\n",
    "\n",
    "    r2,rmse = evaluation(y_val_1, y_CNN_hat) #验证集的拟合优度\n",
    "    R21=r2_score(y_val_1, y_CNN_hat)\n",
    "    print(i,R21)\n",
    "    r2_CNN_val.append(R21)\n",
    "\n",
    "    test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_CNN_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    r22,rmse = evaluation(y_test, y_CNN_pred)\n",
    "    print(\"r2:\",r22,\"rmse:\",rmse)\n",
    "    r2_CNN_test.append(r22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678042c",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_CNN_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496a5c6",
   "metadata": {},
   "source": [
    "# 对比模型一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#单尺度模型\n",
    "####这里这里这里！！！！！！！！！！\n",
    "from tensorflow.keras import optimizers # 错误代码1的改正方案\n",
    "#模型1:单尺度融合模型\n",
    "def single_channel_merge_model(demgras_dim):\n",
    "    print('开始模型1:单尺度融合模型')\n",
    "    codes_in = Input(shape=(MAX_LEN, ), dtype='float32') #MAX_len 输入句子的最大长度17\n",
    "    if args.init:\n",
    "        print('initialize embedding layer with pre-training vectors')\n",
    "        print('依据预训练向量 初始化嵌入层')\n",
    "        print('embedding layers trainalble %s' % args.trainable) \n",
    "        embedding0level = Embedding(output_dim=embedding_vector_length,  #424\n",
    "                                input_dim=max_features, \n",
    "                                input_length=MAX_LEN,\n",
    "                                weights=[embedding_matrix0],\n",
    "                                trainable=args.trainable)(codes_in)\n",
    "    else:\n",
    "        print('one hot embedding with random initialization...') \n",
    "        embedding0level = Embedding(output_dim=embedding_vector_length,\n",
    "                                    input_dim=max_features,\n",
    "                                    embeddings_initializer='random_uniform',\n",
    "                                    input_length=MAX_LEN)(codes_in)\n",
    "    conv_result = []\n",
    "    for i in range(3):\n",
    "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
    "        conv0 = conv_layer(embedding0level)\n",
    "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
    "        conv_result.append(pooling0)\n",
    "\n",
    "    print('添加人口统计信息，融合')\n",
    "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
    "    dense_demgras = Dense(3, activation='sigmoid')(demgras_in)\n",
    "    conv_result.append(dense_demgras)#append the demographics information\n",
    "    merge_out = concatenate(conv_result)\n",
    "    dense_out = Dense(500, activation='relu')(merge_out)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(100, activation='relu')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    mode_out = Dense(1)(dpt)\n",
    "    model = Model([codes_in, demgras_in], mode_out)\n",
    "    #rmsprop = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 出错代码1\n",
    "    #rmsprop = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) #weight_decay的作用是正则化\n",
    "    rmsprop = tf.keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mae']) #MSE均方误差：真实值-预测值 然后平方之后求和平均 #MAE：平均绝对误差\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "#模型2:单尺度划分模型\n",
    "def single_channel_split_model(demgras_dim):\n",
    "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
    "    if args.init:\n",
    "        print('initialize embedding layer with pre-training vectors')\n",
    "        print('依据预训练向量，初始化嵌入层...')\n",
    "        print('embedding layers trainalble %s' % args.trainable) \n",
    "        dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
    "                                        input_dim=max_features,\n",
    "                                        input_length=DIS_MAX_LEN,\n",
    "                                        weights=[embedding_matrix0],\n",
    "                                        trainable=args.trainable)(dis_in)\n",
    "\n",
    "    else:\n",
    "        print('one hot embedding with random initialization...')\n",
    "        print('随机初始化,独热嵌入层...')\n",
    "        dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                embeddings_initializer='random_uniform')(dis_in)\n",
    "\n",
    "    conv_result = []\n",
    "    for i in range(3):\n",
    "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
    "        conv0 = conv_layer(dis_embedding0level)\n",
    "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
    "        conv_result.append(pooling0)\n",
    "\n",
    "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
    "    dense_demgras = Dense(3, activation='sigmoid')(demgras_in)\n",
    "    conv_result.append(dense_demgras)#append the demographics information\n",
    "    merge_out = concatenate(conv_result)\n",
    "    dense_out = Dense(1000, activation='relu')(merge_out)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(500, activation='relu')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(100, activation='relu')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    mode_out = Dense(1)(dpt)\n",
    "    model = Model([dis_in,  demgras_in], mode_out) #sur_in,\n",
    "    rmsprop = tf.keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mae'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "#第三个模型：多尺度划分模型\n",
    "def multi_channel_split_model(demgras_dim):\n",
    "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
    "    dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix0],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding1level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix1],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding2level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix2],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "    dis_embedding3level = Embedding(output_dim=embedding_vector_length,\n",
    "                                input_dim=max_features,\n",
    "                                input_length=DIS_MAX_LEN,\n",
    "                                weights=[embedding_matrix3],\n",
    "                                trainable=args.trainable)(dis_in)\n",
    "\n",
    "    conv_result = []\n",
    "    for i in range(3):\n",
    "        channel_result = []\n",
    "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
    "        conv0 = conv_layer(dis_embedding0level)\n",
    "        conv1 = conv_layer(dis_embedding1level)\n",
    "        conv2 = conv_layer(dis_embedding2level)\n",
    "        conv3 = conv_layer(dis_embedding3level)\n",
    "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
    "        pooling1 = GlobalMaxPooling1D()(conv1)\n",
    "        pooling2 = GlobalMaxPooling1D()(conv2)\n",
    "        pooling3 = GlobalMaxPooling1D()(conv3)\n",
    "        channel_result.append(pooling0)\n",
    "        channel_result.append(pooling1)\n",
    "        channel_result.append(pooling2)\n",
    "        channel_result.append(pooling3)\n",
    "        allchannel = add(channel_result)\n",
    "        conv_result.append(allchannel)\n",
    "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
    "    print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:11\n",
    "    print(\"demgras_in.shape:{}\".format(demgras_in.shape))#(None, 11)\n",
    "    dense_demgras = Dense(1000, activation='sigmoid')(demgras_in)\n",
    "    conv_result.append(dense_demgras)#append the demographics information\n",
    "    merge_out = concatenate(conv_result) #numpy.concatenate函数 主要作用:沿现有的某个轴对一系列数组进行拼接。\n",
    "    dense_out = Dense(1000, activation='relu', name='F1')(merge_out)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(500, activation='relu', name='F2')(dpt) \n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(100, activation='relu', name='F3')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(50, activation='relu', name='F4')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    dense_out = Dense(2, activation='relu', name='F5')(dpt)\n",
    "    dpt = Dropout(dropout)(dense_out)\n",
    "    mode_out = Dense(1)(dpt)\n",
    "    model = Model([dis_in,demgras_in], mode_out) #sur_in, \n",
    "    #rmsprop = optimizers.rmsprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 均方根传播(RMSProp) ;根据最近的权重梯度的平均值(例如变化的速度)来调整;这意味着该算法在线上和非平稳问题上表现良好(如:噪声)。\n",
    "    Adam=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae']) #rmsprop\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train):\n",
    "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
    "\n",
    "    print('Training the merge model....')\n",
    "    model.fit([X_train, demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[checkpointer,earlystopper],\n",
    "              verbose=1)\n",
    "\n",
    "def train_single_channel_split_model(model, modelpath, X1_train, demographics_train, y_train):\n",
    "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
    "\n",
    "    print('Training the merge model....') \n",
    "    model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[checkpointer,earlystopper],\n",
    "              verbose=1)\n",
    "\n",
    "def train_multi_channel_split_model(model, modelpath, X1_train,  demographics_train, y_train):\n",
    "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
    "\n",
    "    print('Training the merge model....') \n",
    "    model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[checkpointer,earlystopper],\n",
    "              verbose=1) \n",
    "\n",
    "def test_single_channel_merge_model(model, modelpath,X_test,demographics_test,y_test, index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    print(r2)\n",
    "    name = 'SG_merge%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "def test_single_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X1_test,  demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    name = 'SG_split%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "def test_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    print(\"y_pred:{}\".format(y_pred.shape))\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    name = 'MG_split%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_SG_merge=[]\n",
    "seed = 1234\n",
    "\n",
    "def test_single_channel_merge_model(model, modelpath,X_test,demographics_test,y_test, index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    print(r2)\n",
    "    r2_SG_merge.append(r2)\n",
    "    name = 'SG_merge%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "r2_SG_split=[]\n",
    "def test_single_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X1_test,  demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    r2_SG_split.append(r2)\n",
    "    name = 'SG_merge%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "r2_MG_merge=[]\n",
    "def test_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
    "    print('Testing model...') \n",
    "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
    "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
    "    print(\"y_pred:{}\".format(y_pred.shape))\n",
    "    r2,rmse = evaluation(y_test, y_pred)\n",
    "    r2_MG_merge.append(r2)\n",
    "    name = 'MG_split%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "for i in range(1):\n",
    "    modelpath = 'SG_merge' + path\n",
    "    model = single_channel_merge_model(demgras_dim)\n",
    "    if not args.test:\n",
    "        train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train)\n",
    "    print('testing on the testing datasets.....')\n",
    "    print(\"modelpath:\",modelpath)\n",
    "    test_single_channel_merge_model(model, modelpath, X_test, demographics_test, y_test, 3)\n",
    "    print('testing on the filtered testing datasets.....')\n",
    "    print(r2_SG_merge)\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    modelpath = 'SG_split' + path\n",
    "    model = single_channel_split_model(demgras_dim)\n",
    "    if not args.test:\n",
    "        train_single_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
    "    print(f,'testing on the testing datasets.....')\n",
    "    test_single_channel_split_model(model, modelpath, X1_test,  demographics_test,y_test,3)\n",
    "    print('testing on the filtered testing datasets.....')\n",
    "    print(r2_SG_split)\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    modelpath = 'MG_merge' + path\n",
    "    model= multi_channel_split_model(demgras_dim)\n",
    "    if not args.test:\n",
    "        train_multi_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
    "    extract_patientvec(model, modelpath, disease, demographics)\n",
    "    print(f,'testing on the testing datasets.....')\n",
    "    test_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
    "    print(r2_MG_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d42d43",
   "metadata": {},
   "source": [
    "# 对比模型2:传统机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================第一部分 独热编码 ============================================\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "import xgboost as xgb\n",
    "def onehot_RF(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    #one_hot_encoder\n",
    "    print('RandomForestRegressor with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = ensemble.RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_SVR(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('SVR with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = svm.SVR()\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...') \n",
    "\n",
    "def onehot_AdaBoost(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('AdaBoost with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_GradientBoosting(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('GradientBoosting with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_Bagging(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('Bagging with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = BaggingRegressor(n_estimators=10,random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_ExtraTree(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('ExtraTree with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = ExtraTreeRegressor(random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_DecisionTree(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('DecisionTree with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = tree.DecisionTreeRegressor(random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_KNeighbors(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('KNeighbors with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = neighbors.KNeighborsRegressor()\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_XGBoost(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    print('XGBoost with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    rf = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "def onehot_LR(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
    "    #one_hot_encoder\n",
    "    print('LR with one_hot encoding...') \n",
    "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
    "    #train_mat = onehot_train\n",
    "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
    "    #rf = ensemble.RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "    #rf.fit(train_mat, y_train.ravel())\n",
    "    rf = LinearRegression(normalize=True, n_jobs=-1)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    #test_mat = onehot_test\n",
    "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
    "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse =  evaluation(y_test, y_pred)\n",
    "    print('testing on filter samples...') \n",
    "\n",
    "#============================== 第二部分 预测函数 单尺度、多尺度======================================================\n",
    "\n",
    "\n",
    "\n",
    "def embedding_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #word2vec encoder\n",
    "    print(f,'RandomForestRegressor with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "\n",
    "    rf = ensemble.RandomForestRegressor(n_estimators=10,random_state=90)\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    \n",
    "    #multichannel word2vec embedding encoder\n",
    "    print(f,'RandomForestRegressor with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    rf = ensemble.RandomForestRegressor(n_estimators=10,random_state=90) #n_jobs=-1便是使用全部的CPU\n",
    "    rf.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = rf.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    index = 1\n",
    "    name = 'MG_RF%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "    print('testing on filter testing samples...')\n",
    "\n",
    "    index = 2\n",
    "    name = 'MG_RF%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "    print('testing on filter samples...')\n",
    "\n",
    "    index = 3\n",
    "    name = 'MG_RF%d'%(index)\n",
    "    if args.isdays:\n",
    "        daysplots(y_test, y_pred, r2, rmse, name)\n",
    "    else:\n",
    "        costplots(y_test, y_pred, r2, rmse, name)\n",
    "\n",
    "\n",
    "def embedding_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #word2vec encoder\n",
    "    print(f,'Linear regression with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    print('LR_train_mat')\n",
    "    print(train_mat.shape)\n",
    "    lr = LinearRegression(normalize=True)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "    #multichannel word2vec embedding encoder\n",
    "    print(f,'Linear regression with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = svm.SVR()\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    \n",
    "\n",
    "def embedding_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'SVR with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    print('SVR_train_mat')\n",
    "    lr = svm.SVR()\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'SVR with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = svm.SVR()\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "def embedding_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'AdaBoost with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'AdaBoost with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "def embedding_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'GradientBoosting with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'GradientBoosting with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "\n",
    "def embedding_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'Bagging with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = BaggingRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'Bagging with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = BaggingRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "\n",
    "def embedding_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'ExtraTree with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ExtraTreeRegressor(random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'ExtraTree with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = ExtraTreeRegressor(random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "\n",
    "def embedding_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'DecisionTree with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = tree.DecisionTreeRegressor(random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'DecisionTree with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = tree.DecisionTreeRegressor(random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "\n",
    "def embedding_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'KNeighbors with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = neighbors.KNeighborsRegressor()\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'KNeighbors with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = neighbors.KNeighborsRegressor()\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "\n",
    "def embedding_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
    "    #1、word2vec encoder\n",
    "    print(f,'XGBoost with embedding encoding...')\n",
    "    train_mat = embedding_encoder(X_train, index_embedding)\n",
    "    test_mat = embedding_encoder(X_test, index_embedding)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)\n",
    "    #2、multichannel word2vec embedding encoder\n",
    "    print(f,'XGBoost with multichannel embedding encoding...')\n",
    "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
    "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
    "    lr = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
    "    lr.fit(train_mat, y_train.ravel())\n",
    "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
    "    y_pred = lr.predict(test_mat)\n",
    "    r2, rmse = evaluation(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "onehot_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "\n",
    "embedding_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
    "embedding_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
